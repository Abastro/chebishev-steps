\documentclass{article}

\usepackage{xcolor}
\usepackage{tikz-cd}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% environments
\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem*{example}{Example}
\newtheorem{exercise}{Exercise}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{fact}{Fact}

\newenvironment{claim}[1]{\par\underline{Claim:}\space#1}{\par\smallskip}
\newenvironment{acase}[1]{\par\underline{Case\space#1:}\space}{\par\smallskip}
\newcommand{\astep}[2]{\par\textbf{Step #1: #2}\par\smallskip}

\numberwithin{equation}{section}
% commands
\newcommand{\contra}{\Rightarrow\!\Leftarrow}

\newcommand{\bZ}{\mathbb{Z}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bC}{\mathbb{C}}

\newcommand{\rangewith}[4]{{#2}#1{#3}, \cdots, {#2}#1{#4}}
\newcommand{\funtyp}[3]{#1: & #2 & \rightarrow & #3 \\ }
\newcommand{\fundecl}[2]{& #1 & \mapsto & #2 \\ }

\newcommand{\abs}[1]{\left\lvert{#1}\right\rvert}

\title{k-step decision algorithms}
\author{Junho Lee}

\begin{document}
\pagecolor{white}
\color{black}
\maketitle

It is a hard problem to determine if a number is relational or not.
Algorithmic approach could be of help here -
while there is no way to directly decide relational property,
one could decide if a number is a root of chebyshev polynomial of certain steps.
This write-up suggests multiple decision algoirthms achieving this goal.

\section{Problem Definition}

Given $u$, we are to find the minimal $k$ where $u$ is the root of $s^{\mathbf{n}}_k$.
For this purpose, it helps to consider $s^{\mathbf{n}}_k(u)$ as a function of $\mathbf{n} \in \bZ^{k-1}$.
Hence, we will fix $u$ and denote the chebyshev polynomial as $s_k(\mathbf{n})$ or $s(\mathbf{n})$.
To check for the common cases, we will assume $u^2 \in \bR$.

Under this setting, we are tasked to determine if a multivariate polynomial $s_k(\mathbf{n})$
has a zero within $\bZ^{k-1}$.

\subsection{Prior consideration}

Recall that when $\mathbf{n} = (\mathbf{n}_L, n_i, \mathbf{n}_R)
  = (\mathbf{n}_{L'}, n_{i-1}, n_i, n_{i+1}, \mathbf{n}_{R'})$,
we have
\begin{equation}\label{key_equation}
  s(\mathbf{n}) =
  n_i u \cdot s(\mathbf{n}_L) \cdot s(\mathbf{n}_R)
  - s(\mathbf{n}_{L'}, n_{i-1}, n_{i+1}, \mathbf{n}_{R'}).
\end{equation}

This implies the following properties.
\begin{fact}
  When $n_i = 0$, $s(\mathbf{n}) = - s(\mathbf{n}_{L'}, n_{i-1}, n_{i+1}, \mathbf{n}_{R'})$.
  Hence, if such $\mathbf{n}$ is a zero, there is a shorter sequence which is a zero.
\end{fact}
\begin{fact}
  As $n_i \to \infty$, $s(\mathbf{n}) / (n_i u)$ converges to $s(\mathbf{n}_L) \cdot s(\mathbf{n}_R)$.
\end{fact}

The first property implies that we do not need to check $n_i = 0$ case for a minimal polynomial,
so our search is reduced to finding a root in $\bZ^{k-1} \setminus \{0\}$.

On the other hand, the latter property means the "normalized" term
\[
  \tilde{s}_k(\mathbf{n}) := \frac{s_k(\mathbf{n})}{u^{k-1} n_1 \cdots n_{k-1}}
\]
converges to the product of $\tilde{s}_i$ and $\tilde{s}_{k-i}$ as $n_i \to \infty$.
This indicates that this "Normalized Chebyshev" has a limiting behavior
based on chebyshev polynomials of lower degree.

\newpage

\section{Limiting properties of functions on integer domain}

Let $\bar \bC$ be the extended complex plane.
We may identify $\bZ$ as subset of this plane, which gives its closure $\bar \bZ = \bZ \cup \{\infty\}$.
Since $\bar \bC$ is compact, a closed set $\bar \bZ$ in it is compact as well.
We have the following property for a function with domain $\bar \bZ^k$.

\begin{theorem}
  Suppose $f: \bar \bZ^k \to \bar \bC$ is a continuous map.
  Then, $\abs{f}$ attains a maximum,
  i.e. $\max_{\mathbf{n} \in \bar \bZ^k} \abs{f(\mathbf{n})} \in \bar \bR$ exists.
\end{theorem}
\begin{proof}
  As a (finite) product of compact sets, $\bar \bZ^k$ is compact, so image of it is compact as well.
  Thus, $\abs{f(\bar \bZ^k)}$ is a compact set in $\bar \bR$.
  Consequently, the set $\abs{f(\bar \bZ^k)}$ has a maximum, as desired.
\end{proof}
Note that minimum exists in a similar sense.

\begin{corollary}
  For any rational function $f$ on $\bC$ defined on and restricted to $\bar \bZ^k$ attains such a maximum.
\end{corollary}
\begin{proof}
  It is known that a polynomial $\bC^k \to \bC$ is continuous,
  which still holds if we extend it to $\bar \bC^k \to \bar \bC$.
  Idenfitying $\bar \bC$ as a complex projective place $\bC P^1$,
  a rational function is continuous in a subset of $\bar \bC^k$ where it is defined.
  Therefore, its restriction to $\bar \bZ^k$ should be continuous as well.
\end{proof}

Since the functions we are to deal with are rational,
they all are continuous, so we can apply this theorem to prove the existence of maximum.
Be cautious that the maximum could be attained at $\partial \bZ^k$;
since such points are limit points where values congregate, this can inhibit the termination of the computation.
That this cannot happen should be verified individually.

\begin{definition}
  Weighted continued fraction is given as
  \[
    F_\lambda(n_1, n_2, \cdots, n_k)
    := \frac{s_k(n_1, \cdots, n_k)}{u \cdot s_{k+1}(n_1, \cdots, n_k)}
    = \cfrac{1}{-\lambda (n_k - \cfrac{1}{-\lambda (n_{k-1} - \cfrac{1}{\ddots \frac{1}{-\lambda n_1}})})}.
  \]
\end{definition}

From this definition, it is obvious that
\begin{enumerate}
  \item $F_\lambda(\mathbf{n})$ is a rational function in $(n_1, \cdots, n_k)$.
  \item $F_\lambda(\mathbf{n}) = \infty$ if and only if $s_k(\mathbf{n}) = \infty$.
\end{enumerate}
Hence, this function fits in the desired condition.

\def\ns {\mathbf{n}}
\def\nsL {\mathbf{n}_L}
\def\nsR {\mathbf{n}_R}

Now, we are ready to prove the Theorem 1.2.

(Wrote in overleaf)

The minimum of $ns$ could be found using this criteria,
as it is when $n_i$ is closest to $[\nsL]_\lambda + [\overline{\nsR}]_\lambda$.
Specifically, assuming there was no zero in the previous step,
the zeros of $ns$ is obtained when we exactly have
$n_i = [\nsL]_\lambda + [\overline{\nsR}]_\lambda$.
Hence, bounds on $n_i$ could be given as
\[ \abs{n_i} \leq \max_{\nsL, \nsR} \abs{[\nsL]_\lambda} + \abs{[\overline{\nsR}]_\lambda}. \]
As we can see,
the zeros could be computed once we know the maximum of the continued fraction in the previous step.

This could be extended to other functions like $\tilde{T}$ or $\hat{T}$ as well.

\begin{proposition}
  For $\ns = (\nsL, n_i, \nsR)$, we have
  \begin{enumerate}
    \item $\displaystyle n\tilde{T}^\ns
    = ns^{(\nsR, - \nsL)} \left( 1 - \frac{- [\nsR, -\nsL]_\lambda + [- \overline{\nsL}, \overline{\nsR}]_\lambda}{n_i} \right).$
    \item $\displaystyle
    n\hat{T}^\ns
    = ns^{(\nsR, - \nsL)} \left( 1 + \frac{- [\nsR, -\nsL]_\lambda + [- \overline{\nsL}, \overline{\nsR}]_\lambda}{n_i} \right).$
  \end{enumerate}
\end{proposition}
\begin{proof}
  \begin{align*}
    & \tilde{T}_k^{(n_1, \cdots, n_k)} \\
    & = \left(
      n_i u s^{(n_1, \cdots, n_{i-1})} s^{(n_{i+1}, \cdots, n_k)}
    - s^{(n_1, \cdots, n_{i-2})} s^{(n_{i+1}, \cdots, n_k)}
    - s^{(n_1, \cdots, n_{i-1})} s^{(n_{i+2}, \cdots, n_k)}
    \right) \\
    & - \left(
      n_i u s^{(n_2, \cdots, n_{i-1})} s^{(n_{i+1}, \cdots, n_{k-1})}
    - s^{(n_2, \cdots, n_{i-2})} s^{(n_{i+1}, \cdots, n_{k-1})}
    - s^{(n_2, \cdots, n_{i-1})} s^{(n_{i+2}, \cdots, n_{k-1})}
    \right) \\
    & = n_i u \cdot P(i-1, i+1) - P(i-2, i+1) - P(i-1, i+2)
  \end{align*}
  where $P(j, l) := s^{(n_1, \cdots, n_j)} s^{(n_l, \cdots, n_k)} - s^{(n_2, \cdots, n_j)} s^{(n_l, \cdots, n_{k-1})}$.

  Observe that
  \begin{align*}
    P(j, l) & = K(n_1 u, \cdots, n_j u) K(n_l u, \cdots, n_k u) - K(n_2 u, \cdots, n_j u) K(n_l u, \cdots, n_{k-1} u) \\
    & = K(n_j u, \cdots, n_1 u) K(n_k u, \cdots, n_l u) - K(n_j u, \cdots, n_2 u) K(n_{k-1} u, \cdots, n_l u) \\
    & = K(n_j u, \cdots, n_1 u, n_k u, \cdots, n_l u)
  \end{align*}
  follows from the Euler identity.
  Therefore,
  \begin{align*}
    \tilde{T}^{(n_1, \cdots, n_k)}
    & = n_i u s^{(n_{i-1}, \cdots, n_1, n_k, \cdots, n_{i+1})}
    - s^{(n_{i-2}, \cdots, n_1, n_k, \cdots, n_{i+1})}
    - s^{(n_{i-1}, \cdots, n_1, n_k, \cdots, n_{i+2})} \\
    & = u s^{(\nsR, \nsL)}
    \left( n_i + [n_{i+1}, \cdots, n_k, n_1, \cdots, n_{i-1}]_\lambda
    + [n_{i-1}, \cdots, n_1, n_k, \cdots, n_{i+1}]_\lambda \right) \\
    & = u s^{(\nsR, \nsL)} \left( n_i + [\nsR, \nsL]_\lambda + [\overleftarrow{\nsL}, \overleftarrow{\nsR}]_\lambda \right)
  \end{align*}
  as desired.

  Similarly for $\hat{T}$, one has
  \[
    \hat{T}_k^\ns = n_i u \cdot Q(i-1, i+1) - Q(i-2, i+1) - Q(i-1, i+2)
  \]
  where $Q(j, l) = s^{(n_1, \cdots, n_j)} s^{(n_l, \cdots, n_k)} + s^{(n_2, \cdots, n_j)} s^{(n_l, \cdots, n_{k-1})}$.
  By inverting some of $n_*$'s signs and applying Euler identity,
  \begin{align*}
    Q(j, l) & = K(n_1 u, \cdots, n_j u) K(n_l u, \cdots, n_k u) + K(n_2 u, \cdots, n_j u) K(n_l u, \cdots, n_{k-1} u) \\
    & = (-1)^j ( K(- n_j u, \cdots, - n_1 u) K(n_k u, \cdots, n_l u) - K(-n_j u, \cdots, -n_2 u) K(n_{k-1} u, \cdots, n_l u) ) \\
    & = (-1)^j K(- n_j u, \cdots, - n_1 u, n_k u, \cdots, n_l u).
  \end{align*}
  Hence, one can follow the similar process as $\tilde{T}$ to obtain
  \begin{align*}
    \hat{T}^\ns = (-1)^{i-1} u s^{(\nsR, -\nsL)}
    \left( n_i - [\nsR, -\nsL]_\lambda + [-\overleftarrow{\nsL}, \overleftarrow{\nsR}]_\lambda \right).
  \end{align*}
\end{proof}

Therefore, under the assumption that the previous steps did not give a zero,
we can obtain the zero by looking for
\begin{align*}
  \tilde{T} = 0: &
  \quad n_i = - [\nsR, -\nsL]_\lambda + [- \overline{\nsL}, \overline{\nsR}]_\lambda, \\
  \hat{T} = 0: &
  \quad n_i = [\nsR, -\nsL]_\lambda - [- \overline{\nsL}, \overline{\nsR}]_\lambda.
\end{align*}

In both cases, we obtain
\[
  \abs{n_i}
  \leq \max_{\nsL, \nsR} \abs{- [\nsR, -\nsL]_\lambda + [- \overline{\nsL}, \overline{\nsR}]_\lambda}
  \leq \max_{\nsL, \nsR} \abs{[\nsR, -\nsL]_\lambda} + \max_{\nsL, \nsR} \abs{[- \overline{\nsL}, \overline{\nsR}]_\lambda}.
\]
which is the range to search zeros for.
Since there are only finitely many such $\ns$ within this bound,
this gives an algorithm to search for the zeros of $\tilde{T}$ and $\hat{T}$.

\begin{algorithm}
  \caption{Zero of normalized T-tilde polynomial}
  \begin{algorithmic}
    \Function{ContinuantMax}{$\lambda$, $k$}
    \EndFunction

    \Function{NormalizedTTildeZero}{$\lambda$, $k$}
      \While{has $\mathbf{n}$ to check}
        \For{$i \in [1, k]$}
          \State $R \gets$ $2$ \Call{ContinuantMax}{$\lambda$, $k-1$}
          \State choose next $n_i \in [- R, R] \setminus \{0\}$
        \EndFor
        \If{$n\tilde{T}_k^\mathbf{n}(u) = 0$}
          \State \textbf{return} $\mathbf{n}$
        \EndIf
      \EndWhile
      \State \textbf{return} "No zero found"
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\newpage

\subsection{Greedy Algorithm}
Based on the observations with the exhaustive algorithm, greedy algorithm was constructed.
Recall that to obtain maximum of $[\ns]_\lambda$, we needed $n_i \approx [\nsL]_\lambda + [\nsR]_\lambda$.
It turned out through experimentations that $\abs{[\nsR]_\lambda} < 1$ holds in most cases.
Using this observation, we can simply construct a greedy algorithm by selecting:

\[
  n_i = \operatorname{round} ([\nsL]_\lambda).
\]
for all but first a few $n_i$'s, for which we give a restriction $\abs{n_i - [\nsL]_\lambda} \leq W$.
That is, given a threshold $N > 0$ and width $W$, take
\[
  \begin{cases}
    \abs{n_i - [\nsL]_\lambda} \leq W, & i \leq N \\
    n_i = \operatorname{round} ([\nsL]_\lambda), & i > N. \\
  \end{cases}
\]
This way, we are guaranteed to find (or fail to find) a zero after $W^N$ tries.
Furthermore, we can search until desired steps in one go, without checking for individual cases.


\end{document}